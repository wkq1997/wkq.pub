---
id: index
title: 概述
---

**架构并不是被“发明”出来的，而是持续演进的结果**

如何用一些不可靠的部件（程序、硬件、网络......）构建出一个可靠、大型的系统，这个设计思想就需要向大自然要答案、抄作业。


:::tip

生命系统之所以可靠的本质，恰恰是因为它可以使用不可靠的部件来完成遗传迭代。这其中的关键点，便是承认细胞、分子这些零部件可能会出错，某个具体的的零部件可能会崩溃消亡，但一定有其后代的出现，重新代替该零部件的作用，以维持系统的整体稳定性。<br/>
流水不腐，有老朽、有消亡、有重生、有更迭，才是正常生态的合理运作规律。

:::

再来看服务架构演进史，软件架构风格从大型机开始、到原始分布式、到大型单体、到面向服务，到微服务，到云原生（服务网格），到无服务，技术架构上都呈现出“从大到小”的发展趋势。 这说明架构演变的最终也是最重要的驱动力，或者说产生这种“从小到大”趋势的最根本的驱动力，始终都是**为了方便某个服务能够顺利的死去与重生而设计的**。

:::warning

不管是软件系统，还是万事万物，其实都可以从生命系统或者从生态系统中理解，生物群就是大型的分布式系统，每一个种群就是一种具体的服务集群，食物链就是服务集群之间的调用关系。种群中的每一个个体就是单体服务。<br/>
自然选择没有目的，只有优胜劣汰，种群必须保证一直在在复制（多生后代），演进（DNA变异），反脆弱（个体联合成集体）。种群可能会消亡（报废），要想食物链不断（雪崩效应），必须有新的物种填补位置（重生）。<br/>
个体的生老病死对整个系统而言，没有影响。

:::

:::note

单体系统每一个部件都不具备涅槃重生的特性，整体上的生命力不如如生态系统一样健壮的分布式系统，这也是微服务逐渐替代单体系统的根本原因。


:::

所以分布式系统设计和实践需要转变思想: 

* **把出错看作是正常、甚至是必须的发展过程，只要出了问题能够兜底，能重回正轨就好。**<br/>
* 系统在设计阶段（而不是在部署运行之后），就需要系统性地考虑到它的每一个组成部分的隔离、出错、容错、崩溃、演进、报废与外部观测等属性。

## CAP 定理
在分布式的环境下设计和部署系统，有 3 个核心的系统需求。



CAP 定理描述了一个分布式的系统中，当涉及到共享数据问题时，三个需求最多只能满足其中两个。

![b15c388bef74f34cc63f1b5818b32c1d](https://img.wkq.pub/hexo/b15c388bef74f34cc63f1b5818b32c1d.webp)

1. 一致性（Consistency）：代表在任何时刻、任何分布式节点中，我们所看到的数据都是没有矛盾的，这里的 C  和 ACID 中的 C 是相同的单词，但它们又有不同的定义（分别指 Replication 的一致性和数据库状态的一致性）。
2. 可用性 （Availability）：代表系统不间断的提供服务的能力,Availability 与本次响应时间成反比。
3. 分区容忍性（Partition Tolerance）：代表在分布式环境中，当部分节点因网络故障因网络原因而彼此失联（即与其它节点形成网络分区）时，系统仍能正确地提供服务的能力。

:::tip

一致性是对系统读写操作获得结果的一种描述！
* 强一致性是指对于系统的任何读写操作都和单机是一样的。
* 最终一致性是指，对一个系统的写操作之后，并不是立刻对所有的读操作生效，即有可能读取到写之前操作的结果。但经过有限的事件，整个系统最终会实现这次写入对所有的读操作可见。

:::

:::tip CP示例
MySQL Cluster 进行全同步复制时，所有 Slave 节点的 Binlog 都完成写入后，Master 的事务才会进行提交。数据一致性得到了满足，但可用性不足
:::


:::tip AP示例

:::


## 分布式共识算法

